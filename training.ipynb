{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWx6_x5-rmYs",
        "outputId": "9e64578b-294f-4f74-f03e-9c6315d6ae45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 706 images belonging to 8 classes.\n",
            "Found 87 images belonging to 8 classes.\n",
            "Epoch 1/30\n",
            "70/70 [==============================] - 405s 5s/step - loss: 1.9152 - accuracy: 0.2529 - val_loss: 1.5192 - val_accuracy: 0.4750\n",
            "Epoch 2/30\n",
            "70/70 [==============================] - 78s 1s/step - loss: 1.1493 - accuracy: 0.5675 - val_loss: 1.1070 - val_accuracy: 0.5250\n",
            "Epoch 3/30\n",
            "70/70 [==============================] - 81s 1s/step - loss: 0.7616 - accuracy: 0.6710 - val_loss: 0.5705 - val_accuracy: 0.7875\n",
            "Epoch 4/30\n",
            "70/70 [==============================] - 79s 1s/step - loss: 0.6502 - accuracy: 0.7241 - val_loss: 0.6278 - val_accuracy: 0.7000\n",
            "Epoch 5/30\n",
            "70/70 [==============================] - 79s 1s/step - loss: 0.6098 - accuracy: 0.7399 - val_loss: 0.6285 - val_accuracy: 0.7125\n",
            "Epoch 6/30\n",
            "70/70 [==============================] - 80s 1s/step - loss: 0.5615 - accuracy: 0.7572 - val_loss: 0.5426 - val_accuracy: 0.7500\n",
            "Epoch 7/30\n",
            "70/70 [==============================] - 85s 1s/step - loss: 0.4961 - accuracy: 0.7830 - val_loss: 0.5567 - val_accuracy: 0.7750\n",
            "Epoch 8/30\n",
            "70/70 [==============================] - 84s 1s/step - loss: 0.4614 - accuracy: 0.8118 - val_loss: 0.4363 - val_accuracy: 0.8000\n",
            "Epoch 9/30\n",
            "70/70 [==============================] - 80s 1s/step - loss: 0.4207 - accuracy: 0.8190 - val_loss: 0.3601 - val_accuracy: 0.8375\n",
            "Epoch 10/30\n",
            "70/70 [==============================] - 77s 1s/step - loss: 0.4255 - accuracy: 0.8319 - val_loss: 0.3378 - val_accuracy: 0.8500\n",
            "Epoch 11/30\n",
            "70/70 [==============================] - 80s 1s/step - loss: 0.4050 - accuracy: 0.8276 - val_loss: 0.6295 - val_accuracy: 0.7375\n",
            "Epoch 12/30\n",
            "70/70 [==============================] - 80s 1s/step - loss: 0.3984 - accuracy: 0.8463 - val_loss: 0.3898 - val_accuracy: 0.8000\n",
            "Epoch 13/30\n",
            "70/70 [==============================] - 80s 1s/step - loss: 0.4662 - accuracy: 0.7874 - val_loss: 0.4897 - val_accuracy: 0.8125\n",
            "Epoch 14/30\n",
            "70/70 [==============================] - 81s 1s/step - loss: 0.3762 - accuracy: 0.8549 - val_loss: 0.2841 - val_accuracy: 0.9250\n",
            "Epoch 15/30\n",
            "70/70 [==============================] - 79s 1s/step - loss: 0.3386 - accuracy: 0.8563 - val_loss: 0.5223 - val_accuracy: 0.8250\n",
            "Epoch 16/30\n",
            "70/70 [==============================] - 83s 1s/step - loss: 0.3404 - accuracy: 0.8664 - val_loss: 0.5788 - val_accuracy: 0.7625\n",
            "Epoch 17/30\n",
            "70/70 [==============================] - 80s 1s/step - loss: 0.3494 - accuracy: 0.8750 - val_loss: 0.2959 - val_accuracy: 0.9375\n",
            "Epoch 18/30\n",
            "70/70 [==============================] - 80s 1s/step - loss: 0.2808 - accuracy: 0.8994 - val_loss: 0.3435 - val_accuracy: 0.8625\n",
            "Epoch 19/30\n",
            "70/70 [==============================] - 80s 1s/step - loss: 0.2684 - accuracy: 0.8894 - val_loss: 0.3213 - val_accuracy: 0.8500\n",
            "Epoch 20/30\n",
            "70/70 [==============================] - 78s 1s/step - loss: 0.2800 - accuracy: 0.8836 - val_loss: 0.2985 - val_accuracy: 0.8250\n",
            "Epoch 21/30\n",
            "70/70 [==============================] - 79s 1s/step - loss: 0.2722 - accuracy: 0.8951 - val_loss: 0.2819 - val_accuracy: 0.9000\n",
            "Epoch 22/30\n",
            "70/70 [==============================] - 80s 1s/step - loss: 0.2400 - accuracy: 0.9152 - val_loss: 0.3120 - val_accuracy: 0.8625\n",
            "Epoch 23/30\n",
            "70/70 [==============================] - 79s 1s/step - loss: 0.2829 - accuracy: 0.8994 - val_loss: 0.3359 - val_accuracy: 0.8500\n",
            "Epoch 24/30\n",
            "70/70 [==============================] - 81s 1s/step - loss: 0.2649 - accuracy: 0.8966 - val_loss: 0.3078 - val_accuracy: 0.8500\n",
            "Epoch 25/30\n",
            "70/70 [==============================] - 80s 1s/step - loss: 0.2493 - accuracy: 0.9080 - val_loss: 0.2393 - val_accuracy: 0.9500\n",
            "Epoch 26/30\n",
            "70/70 [==============================] - 78s 1s/step - loss: 0.2047 - accuracy: 0.9210 - val_loss: 0.2883 - val_accuracy: 0.9000\n",
            "Epoch 27/30\n",
            "70/70 [==============================] - 78s 1s/step - loss: 0.2065 - accuracy: 0.9267 - val_loss: 0.3030 - val_accuracy: 0.9125\n",
            "Epoch 28/30\n",
            "70/70 [==============================] - 79s 1s/step - loss: 0.2287 - accuracy: 0.8994 - val_loss: 0.2692 - val_accuracy: 0.8875\n",
            "Epoch 29/30\n",
            "70/70 [==============================] - 77s 1s/step - loss: 0.1963 - accuracy: 0.9167 - val_loss: 0.2108 - val_accuracy: 0.9250\n",
            "Epoch 30/30\n",
            "70/70 [==============================] - 79s 1s/step - loss: 0.1964 - accuracy: 0.9310 - val_loss: 0.2656 - val_accuracy: 0.8750\n",
            "Found 92 images belonging to 8 classes.\n",
            "9/9 [==============================] - 44s 5s/step - loss: 0.2828 - accuracy: 0.8556\n",
            "Test Loss: 0.2828\n",
            "Test Accuracy: 0.8556\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Set GPU device\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "# Set the path to your tea leaf dataset\n",
        "train_dir = '/content/drive/MyDrive/tea_leaf/ratio3:1/train'\n",
        "valid_dir = '/content/drive/MyDrive/tea_leaf/ratio3:1/val'\n",
        "test_dir = '/content/drive/MyDrive/tea_leaf/ratio3:1/test'\n",
        "\n",
        "# Set the image dimensions\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# Set the number of classes (disease categories)\n",
        "num_classes = 8\n",
        "\n",
        "# Set the batch size and number of epochs\n",
        "batch_size = 10\n",
        "epochs = 30\n",
        "\n",
        "# Create data generators with data augmentation for training and validation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2\n",
        ")\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    valid_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load the pre-trained VGG16 model without the top layer\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "# Add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add a fully connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "# Add the final classification layer\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Unfreeze the last few layers of the VGG16 model for fine-tuning\n",
        "for layer in model.layers[-10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "with tf.device('/GPU:0'):\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // batch_size,\n",
        "        validation_data=valid_generator,\n",
        "        validation_steps=valid_generator.samples // batch_size,\n",
        "        epochs=epochs\n",
        "    )\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "    loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
        "print(f'Test Loss: {loss:.4f}')\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/tea_leaf/tea_leaf_model.h5')"
      ],
      "metadata": {
        "id": "SOBoNn7pxceg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "TNOuC4TUxgjt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "model=load_model('/content/tea_leaf_model.h5')"
      ],
      "metadata": {
        "id": "hJ35rEXaxi1N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img=image.load_img(r'/content/drive/MyDrive/tea_leaf/train/Anthracnose/IMG_20220503_143344.jpg',target_size=(224,224))\n",
        "#convert image to array format\n",
        "x=image.img_to_array(img)\n",
        "import numpy as np\n",
        "x=np.expand_dims(x,axis=0)\n",
        "img_data = preprocess_input(x)\n",
        "output=np.argmax(model.predict(img_data), axis=1)\n",
        "index=['Anthracnose','algal leaf','bird eye spot','brown blight','gray light','healthy','red leaf spot','white spot']\n",
        "result = str(index[output[0]])\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "KC-AUOwoxlYA",
        "outputId": "dd7f7bb0-487b-4fad-bec1-cc9298826fb1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 733ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Anthracnosealgal leaf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}